{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5497e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea06a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "raw_dataset = pd.read_csv(\"data/asteroids.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5438eaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset[\"diameter_missing\"] = raw_dataset[\"diameter\"].isna().astype(int)\n",
    "raw_dataset[\"pha\"] = raw_dataset[\"pha\"].map({\"Y\": 1.0, \"N\" : 0.0})\n",
    "raw_dataset = raw_dataset[raw_dataset[\"pha\"].isin([1.0,0.0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e899a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = raw_dataset[raw_dataset[\"neo\"] == \"Y\"]\n",
    "\n",
    "features = [\"H\", \"epoch\", \"diameter_missing\", \"epoch_mjd\", \"e\", \"a\", \"q\", \"i\", \"om\", \"w\", \"rms\", \"per\", \"per_y\"]\n",
    "target = \"pha\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a20c753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis\n",
    "\n",
    "# print(dataset[target].info())\n",
    "# print(dataset.describe())\n",
    "# print(dataset[\"pha\"].value_counts())\n",
    "\n",
    "# dataset[\"pha\"].value_counts().plot(kind='bar')\n",
    "# plt.title(\"PHA vs Non-PHA Asteroids\")\n",
    "# plt.xlabel(\"PHA\")\n",
    "# plt.ylabel(\"Count\")\n",
    "\n",
    "# X.hist(bins=30, figsize=(10,8))\n",
    "# plt.suptitle(\"Feature Distributions\")\n",
    "# plt.show()\n",
    "\n",
    "# import seaborn as sns\n",
    "\n",
    "# for col in [\"diameter\", \"albedo\", \"H\", \"e\", \"a\"]:\n",
    "#     sns.boxplot(data=dataset, x=\"pha\", y=col)\n",
    "#     plt.title(f\"{col} vs PHA\")\n",
    "#     plt.show()\n",
    "\n",
    "# sns.pairplot(dataset, vars=[\"diameter\", \"H\", \"albedo\"], hue=\"pha\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a70c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Data\n",
    "\n",
    "train_df, test_df = train_test_split(dataset, test_size=0.4, random_state=42)\n",
    "test_df, val_df = train_test_split(train_df, test_size=0.5, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "60400860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute Missing Values\n",
    "\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "train_df[features] = imputer.fit_transform(train_df[features])\n",
    "val_df[features]    = imputer.transform(val_df[features])\n",
    "test_df[features]   = imputer.transform(test_df[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "a5575c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale Features\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "train_df[features] = scaler.fit_transform(train_df[features])\n",
    "val_df[features] = scaler.transform(val_df[features])\n",
    "test_df[features] = scaler.transform(test_df[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "387ab506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset & DataLoader\n",
    "\n",
    "class AsteroidDataset(Dataset):\n",
    "    def __init__(self, df, feature_cols, target_col):\n",
    "        self.X = torch.tensor(df[feature_cols].values, dtype=torch.float32)\n",
    "        self.y = torch.tensor(df[target_col].values, dtype=torch.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "train_dataset = AsteroidDataset(train_df, features, target)\n",
    "test_dataset = AsteroidDataset(test_df, features, target)\n",
    "val_dataset = AsteroidDataset(val_df, features, target)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "5df0c175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(dim, dim)\n",
    "        self.bn = nn.LayerNorm(dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(dim, dim)\n",
    "        self.bn2 = nn.LayerNorm(dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc(x)\n",
    "        out = self.bn(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        return self.bn2(out + x)\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=256, num_blocks=3, output_dim=1, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.bn = nn.LayerNorm(hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.blocks = nn.Sequential(*[ResidualBlock(hidden_dim, dropout) for _ in range(num_blocks)])\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.bn(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.blocks(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "74636fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Setup\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = Network(len(features)).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "c2ad2a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_weight = train_df[train_df[\"pha\"] == 0.0].size / train_df[train_df[\"pha\"] == 1.0].size\n",
    "pos_weight = torch.tensor([pos_weight]).to(device)\n",
    "# print(train_df)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "d3d78bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation\n",
    "\n",
    "def validate():\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in val_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            logits = model(X)\n",
    "            loss = criterion(logits.squeeze(), y)\n",
    "            total_loss += loss.item() * X.size(0)\n",
    "\n",
    "            preds = (torch.sigmoid(logits) > 0.5).float()\n",
    "            correct += (preds.squeeze() == y).sum().item()\n",
    "            total += y.size(0)\n",
    "\n",
    "    print(f\"Validation Loss: {total_loss/total:.4f} | Validation Accuracy: {correct/total:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "a4f99119",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Target size (torch.Size([1])) must be the same as input size (torch.Size([]))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[310], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Train Loss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss\u001b[38;5;241m/\u001b[39mtotal\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Train Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcorrect\u001b[38;5;241m/\u001b[39mtotal\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m         validate()\n\u001b[0;32m---> 26\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[310], line 13\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(epochs)\u001b[0m\n\u001b[1;32m     11\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     12\u001b[0m logits \u001b[38;5;241m=\u001b[39m model(X)\n\u001b[0;32m---> 13\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/Desktop/Development/Data Science/FinalProject/env/lib/python3.9/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Development/Data Science/FinalProject/env/lib/python3.9/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/Development/Data Science/FinalProject/env/lib/python3.9/site-packages/torch/nn/modules/loss.py:828\u001b[0m, in \u001b[0;36mBCEWithLogitsLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 828\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy_with_logits\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    829\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Development/Data Science/FinalProject/env/lib/python3.9/site-packages/torch/nn/functional.py:3593\u001b[0m, in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[0;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[1;32m   3590\u001b[0m     reduction_enum \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[1;32m   3592\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m==\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()):\n\u001b[0;32m-> 3593\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3594\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) must be the same as input size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3595\u001b[0m     )\n\u001b[1;32m   3597\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbinary_cross_entropy_with_logits(\n\u001b[1;32m   3598\u001b[0m     \u001b[38;5;28minput\u001b[39m, target, weight, pos_weight, reduction_enum\n\u001b[1;32m   3599\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Target size (torch.Size([1])) must be the same as input size (torch.Size([]))"
     ]
    }
   ],
   "source": [
    "def train(epochs=20):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for X,y in train_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(X)\n",
    "            loss = criterion(logits.squeeze(), y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Tracking\n",
    "            total_loss += loss.item() * X.size(0)\n",
    "            preds = (torch.sigmoid(logits) > 0.5).float()\n",
    "            correct += (preds.squeeze() == y).sum().item()\n",
    "            total += y.size(0)\n",
    "\n",
    "        print(f\"Epoch {epoch}: Train Loss = {total_loss/total:.4f} | Train Acc: {correct/total:.4f}\")\n",
    "        validate()\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a715fbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            logits = model(X)\n",
    "            loss = criterion(logits.squeeze(), y)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            preds = (torch.sigmoid(logits) > 0.5).float()\n",
    "            \n",
    "            tp += torch.logical_and(y, preds.squeeze()).sum().item()\n",
    "            tn += torch.logical_not(torch.logical_or(y, preds.squeeze())).sum().item()\n",
    "            fp += torch.logical_and(preds.squeeze(), torch.logical_not(y)).sum().item()\n",
    "            fn += torch.logical_and(y, torch.logical_not(preds.squeeze())).sum().item()\n",
    "            \n",
    "            print(f\"Test Metrics: Accuracy={(tp + tn)/(tp + tn + fp + fn):.4f} | Precision={(tp/(tp + fp)):.4f} | Recall={(tp/(fn + tp)):.4f} | F1={tp/(tp + (.5 * (tp + fn))):.4f}\")\n",
    "            \n",
    "test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7383a36",
   "metadata": {},
   "source": [
    "## Handling null values\n",
    "- Create a flag column for diameter missing asteroids\n",
    "- Remove diameter attribute and other columns with null values\n",
    "- dataset.isnull().sum()\n",
    "- diameter, albedo, diameter_sigma\n",
    "\n",
    "## Handling imbalanced target\n",
    "- Try weighting 'Y' class more\n",
    "- Afterwards, observe recall & F1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
